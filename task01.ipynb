{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ltxMnLb3vw"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets # install the datasets package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsj8xE5xo_-c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, DataCollatorForLanguageModeling, create_optimizer, AdamWeightDecay, TrainingArguments\n",
        "from transformers.keras_callbacks import PushToHubCallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPJavENbpCBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08f0a9c-0b9b-45ca-ca93-34199ca11529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAUxFKM-pEbe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], return_special_tokens_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Assume dataset is a DatasetDict\n",
        "tokenized_datasets = DatasetDict()\n",
        "\n",
        "for split in dataset.keys():\n",
        "    # Select the first 500 elements of each split\n",
        "    small_dataset = dataset[split].select(range(500))\n",
        "    # Apply the tokenize function\n",
        "    tokenized_datasets[split] = small_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
      ],
      "metadata": {
        "id": "9jFbwXnot66x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZB6cobepE99"
      },
      "outputs": [],
      "source": [
        "# tokenized_datasets = dataset.select.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdjmThz9pGz5"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    block_size = 128\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i:i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NabUIp7pL8S"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hom7XFudtN5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0274f0f5-4237-4f7a-aa0a-c95bc1191919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into training and validation\n",
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    collate_fn=lambda x: {\n",
        "        \"input_ids\": tf.constant([f['input_ids'] for f in x]),\n",
        "        \"attention_mask\": tf.constant([f['attention_mask'] for f in x]),\n",
        "        \"labels\": tf.constant([f['labels'] for f in x]),\n",
        "    },\n",
        ")\n",
        "\n",
        "val_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    collate_fn=lambda x: {\n",
        "        \"input_ids\": tf.constant([f['input_ids'] for f in x]),\n",
        "        \"attention_mask\": tf.constant([f['attention_mask'] for f in x]),\n",
        "        \"labels\": tf.constant([f['labels'] for f in x]),\n",
        "    },\n",
        ")\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdzTKdPpp1n4"
      },
      "outputs": [],
      "source": [
        "# Define the loss function explicitly\n",
        "def compute_loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1XD__gRp5Al"
      },
      "outputs": [],
      "source": [
        "# Compile the model with a custom loss function\n",
        "optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss=compute_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxV_FWESp_48"
      },
      "outputs": [],
      "source": [
        "# # Custom training loop\n",
        "# epochs = 3\n",
        "# for epoch in range(epochs):\n",
        "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "#     for batch in train_dataset:\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             inputs = {\n",
        "#                 \"input_ids\": batch[\"input_ids\"],\n",
        "#                 \"attention_mask\": batch[\"attention_mask\"],\n",
        "#                 \"labels\": batch[\"labels\"],\n",
        "#             }\n",
        "#             logits = model(inputs, training=True).logits\n",
        "#             loss = compute_loss(inputs[\"labels\"], logits)\n",
        "\n",
        "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
        "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "#         print(f\"Loss: {loss.numpy().mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK04PpZeqi4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1769306-af45-42b3-b236-2440d6e13375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 1.6279491338019187\n",
            "Validation Loss: 0.05509876326790878\n",
            "Epoch 2/5\n",
            "Training Loss: 0.07697614912803356\n",
            "Validation Loss: 0.0516012255102396\n",
            "Epoch 3/5\n",
            "Training Loss: 0.0526533737205542\n",
            "Validation Loss: 0.05032658004867179\n",
            "Epoch 4/5\n",
            "Training Loss: 0.04478792982319227\n",
            "Validation Loss: 0.04883518322770085\n",
            "Epoch 5/5\n",
            "Training Loss: 0.04004255567605679\n",
            "Validation Loss: 0.04724022267120225\n"
          ]
        }
      ],
      "source": [
        "# Early stopping parameters\n",
        "patience = 2  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    for batch in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[\"input_ids\"],\n",
        "                \"attention_mask\": batch[\"attention_mask\"],\n",
        "                \"labels\": batch[\"labels\"],\n",
        "            }\n",
        "            logits = model(inputs, training=True).logits\n",
        "            loss = compute_loss(inputs[\"labels\"], logits)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        train_losses.append(loss.numpy().mean())\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "    print(f\"Training Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Validation loop\n",
        "    val_losses = []\n",
        "    for batch in val_dataset:\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[\"input_ids\"],\n",
        "            \"attention_mask\": batch[\"attention_mask\"],\n",
        "            \"labels\": batch[\"labels\"],\n",
        "        }\n",
        "        logits = model(inputs, training=False).logits\n",
        "        loss = compute_loss(inputs[\"labels\"], logits)\n",
        "        val_losses.append(loss.numpy().mean())\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "    print(f\"Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        model.save_pretrained('best_model')  # Save the model with the best validation loss\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ4hTnZ2RBV1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "# tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4py47SElEDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603ac8fb-9108-4787-97db-beaafd89cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time time time times times times days days days day day day night night night nights nights nights evenings nights nights Nights Nights Nights Night Night Night Star Star StarStarStarStarstarstar star star star stars stars stars Stars Stars Starsstarsstarsstars\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model\n",
        "# model = TFGPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "# Create a text generation pipeline\n",
        "\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_p=0.92,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    num_beams=3,\n",
        "    no_repeat_ngram_size=3,\n",
        "    early_stopping=True,\n",
        "    repetition_penalty=3.0,\n",
        "    length_penalty=1.5,\n",
        "    top_k=50,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "# Generate text based on a prompt\n",
        "prompt = \"Once upon a time\"\n",
        "generated_texts = text_generator(prompt)\n",
        "\n",
        "print(generated_texts[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFzMfYgUqlZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5aa98e-1d88-429c-bc34-53b8c82e767d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I had to go  there with my mates to start our project project project Project Project Project Projects Projects Projects projects projects projectsprojectsprojectsprojectsprojectprojectProjectProjectProjectProProProproproprororororosrosrosososososesosesoses\n"
          ]
        }
      ],
      "source": [
        "prompt = \"I had to go  there with my mates to start our project\"\n",
        "generated_texts = text_generator(prompt)\n",
        "\n",
        "print(generated_texts[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm4nrrGkzxgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0122d58b-301d-460d-c263-5e438952ddcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to go , see you!!!...... : : ::::,,,,\",\",\",',',',...,...,..................................................................................................\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Time to go , see you!\"\n",
        "generated_texts = text_generator(prompt)\n",
        "\n",
        "print(generated_texts[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR3S6Njmz14k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3027ee-557f-4e43-f893-bb8ed1229b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ministery decided to disable the athentication of the papers papers newspapers newspapers newspapers newspaper newspaper newspaper paper paper paper Paper Paper PaperPaperPaperPaperpaperpaperpaperpaperspaperspapersppppppppp pp pp pp p p p P P P\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The ministery decided to disable the athentication of the papers\"\n",
        "generated_texts = text_generator(prompt)\n",
        "\n",
        "print(generated_texts[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fahu8-Elz_MR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}